### General node health ###

groups:
- name: node.alerts
  rules:
  - alert: KubernetesHostHighCPUUsage
    expr: avg(irate(node_cpu{mode="idle"}[5m])) by(instance, region) < 0.2
    for: 3m
    labels:
      tier: kubernetes
      service: node
      severity: warning
      context: node
      meta: "{{ $labels.instance }}"
      dashboard: kubernetes-node?var-server={{$labels.instance}}
    annotations:
      description: High load on node
      summary: The node {{ $labels.instance }} has more than 80% CPU load

  - alert: KubernetesNodeClockDrift
    expr: abs(ntp_drift_seconds) > 0.3
    for: 10m
    labels:
      tier: kubernetes
      service: node
      severity: warning
      context: availability
      meta: "{{ $labels.instance }}"
      dashboard: kubernetes-node?var-server={{$labels.instance}}
    annotations:
      description: High NTP drift
      summary: The clock on node {{ $labels.instance }} is more than 300ms apart from its NTP server. This can cause service degradation in Swift

  - alert: KubernetesNodeKernelDeadlock
    expr: kube_node_status_condition{condition="KernelDeadlock",status="true"} == 1
    for: 96h
    labels:
      tier: kubernetes
      service: node
      severity: info
      context: availability
      meta: "{{ $labels.instance }}"
      playbook: docs/support/playbook/k8s_node_safe_rebooting.html
    annotations:
      description: Node kernel has deadlock
      summary: Permanent kernel deadlock on {{$labels.node}}. Please drain and reboot node

  - alert: KubernetesNodeDiskPressure
    expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
    for: 5m
    labels:
      tier: kubernetes
      service: node
      severity: warning
      context: node
      meta: "{{ $labels.instance }}"
    annotations:
      description: Insufficient disk space
      summary: Node {{$labels.instance}} under pressure due to insufficient available disk space

  - alert: KubernetesNodeMemoryPressure
    expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
    for: 5m
    labels:
      tier: kubernetes
      service: node
      severity: warning
      context: node
      meta: "{{ $labels.instance }}"
    annotations:
      description: Insufficient memory
      summary: Node {{$labels.instance}} under pressure due to insufficient available memory


  ### Network health ###

  - alert: KubernetesNodeHighNumberOfOpenConnections
    expr: node_netstat_Tcp_CurrEstab > 20000
    for: 15m
    labels:
      tier: kubernetes
      service: node
      severity: warning
      context: availability
      meta: "{{ $labels.instance }}"
      dashboard: "kubernetes-node?var-server={{$labels.instance}}"
    annotations:
      description: High number of open TCP connections
      summary: The node {{ $labels.instance }} has more than 20000 active TCP connections. The maximally possible amount is 32768 connections

  - alert: KubernetesNodeHighRiseOfOpenConnections
    expr: predict_linear(node_netstat_Tcp_CurrEstab[20m], 3600) > 32768
    for: 15m
    labels:
      tier: kubernetes
      service: node
      severity: warning
      context: availability
      meta: "{{ $labels.instance }}"
      dashboard: "kubernetes-node?var-server={{$labels.instance}}"
      playbook: "docs/support/playbook/kubernetes/k8s_high_tcp_connections.html"
    annotations:
      description: High number of open TCP connections
      summary: The node {{ $labels.instance }} will likely reach 32768 active TCP connections within the next hour. If that happens, it cannot accept any new connections

  - alert: KubernetesNodeContainerOOMKilled
    expr: sum by (kubernetes_io_hostname) (increase(node_vmstat_oom_kill[10m])) > 0
    labels:
      tier: kubernetes
      service: node
      severity: info
      context: memory
    annotations:
      description: Container was OOM
      summary:  A container on {{ $labels.kubernetes_io_hostname }} was killed after being out of memory
